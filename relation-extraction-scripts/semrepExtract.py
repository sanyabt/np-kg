'''
Script to extract predications from SemRep from processed files generated by semrep_process_pmid.
'''
import sys, os
from datetime import datetime, timedelta
import pandas as pd
import subprocess
from nltk import tokenize
import requests
import time
from pdf_to_text import read_PDF_file, process_PDF_file
import logging

workingDir = os.getcwd()
log_dir = workingDir + '/logs/'

np = []

extraction = True

count_dict = {
			'n_total_pmid': 0,
			'n_success': 0,
			'n_error': 0,
			'n_statements':0,
			'n_files_processed': 0,
			'n_pdf': 0
			}

pub_year_to_pmid_map = {}
pub_type_to_pmid_map = {}

section_tags = ['<ABSTRACT>', '<INTRODUCTION>', '<BACKGROUND>', '<METHODS>', '<RESULT>', '<RESULTS>', '<DISCUSSION>', 
	'<CONCLUSION>', '<CONCLUSIONS>']

def get_publication_year_and_type(pmid):
	pub_year = ''
	pub_type = ''
	if pmid == '':
		return pub_year, pub_type
	if pmid in pub_year_to_pmid_map and pmid in pub_type_to_pmid_map:
		if pub_year_to_pmid_map[pmid] != '':
			pub_year = pub_year_to_pmid_map[pmid]
			pub_type = pub_type_to_pmid_map[pmid]
			return pub_year, str(pub_type)
	time.sleep(5)
	uri = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id="+pmid+"&retmode=json"
	response = requests.get(uri)
	if response.status_code == 429:
		time.sleep(5)
		response = requests.get(uri)
	if response.status_code == 200:
		result = response.json()
		pub_year = result['result'][pmid]['pubdate']
		pub_year_to_pmid_map[pmid] = pub_year

		pub_type = result['result'][pmid]['pubtype']
		pub_type_to_pmid_map[pmid] = pub_type
	return pub_year, str(pub_type)

###counting of statements is incorrect and being done per file not all - last reported is for the file not the entire set in count dict or log
def semrep_extract(filepath):
	result_dict = {
		'index': [],
		'pmid': [],
		'relation': [],
		'year': [],
		'subject_cui': [],
		'object_cui': [],
		'subject_name': [],
		'object_name': [],
		'subject_type': [],
		'object_type': [],
		'sentence': [],
		'source_section': [],
		'pub_type': []
	}
	index = 0
	semrep_files = os.listdir(filepath)
	for file in semrep_files:
		print(file)
		pmid = file.split('.')[0]
		sem_relations = {
			'items': [],
			'source_sentence': [],
			'source_section': []
		}
		with open(filepath+file, 'r', errors='ignore') as file_sem:
			lines = file_sem.readlines()
		
		last_non_empty = ''
		section_match = ''
		for item in lines:
			if any(s in item for s in section_tags):
				#assign section
				section_match = next((sec for sec in section_tags if sec in item), False)
			if '|relation|' in item:
				sem_relations['items'].append(item)
				sem_relations['source_sentence'].append(last_non_empty)
				sem_relations['source_section'].append(section_match)
			elif item == '\n' or item == '':
				continue
			else:
				last_non_empty = item
			
		count_dict['n_statements'] += len(sem_relations)
		for rel in sem_relations['items']:
			fields = rel.split('|')
			if len(fields) < 5:
				continue
			result_dict['index'].append(index)
			result_dict['pmid'].append(pmid)
			result_dict['subject_cui'].append(fields[2])
			result_dict['object_cui'].append(fields[9])
			result_dict['subject_name'].append(fields[3])
			result_dict['object_name'].append(fields[10])
			result_dict['relation'].append(fields[8])
			result_dict['subject_type'].append(fields[4])
			result_dict['object_type'].append(fields[11])
			pub_year, pub_type = get_publication_year_and_type(pmid)
			result_dict['year'].append(pub_year)
			result_dict['pub_type'].append(pub_type)
			relation_index = sem_relations['items'].index(rel)
			result_dict['sentence'].append(sem_relations['source_sentence'][relation_index])
			result_dict['source_section'].append(sem_relations['source_section'][relation_index])
			index += 1

	return result_dict

if __name__ == '__main__':

	for item in np:
		print('Processing ', item)
		outputDir = workingDir + '/output_files/'+item+'/semrepOutput/'
		result_dict = semrep_extract(outputDir)

		semrep_result = pd.DataFrame(result_dict)
		semrep_result_unique = semrep_result.drop_duplicates(subset=['subject_cui', 'subject_name', 'subject_type',
					'relation', 'object_cui', 'object_name', 'object_type', 'year', 'sentence', 'source_section', 'pub_type'])
		semrep_result_unique.to_csv(workingDir+'/output_files/'+item+'/' +item+'_pmid_all_predicates_semrep-extract.tsv', sep='\t', index=False,
					columns=['index', 'pmid', 'subject_cui', 'subject_name', 'subject_type',
					'relation', 'object_cui', 'object_name', 'object_type', 'year', 'sentence', 'source_section', 'pub_type'])
